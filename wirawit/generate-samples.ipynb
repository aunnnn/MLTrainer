{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from shared.models.basic_lstm import BasicLSTM\n",
    "from shared.models.multi_layer_lstm import MultiLayerLSTM\n",
    "from shared.process.pa4_dataloader import build_all_loaders\n",
    "from shared.process.PA4Trainer import get_computing_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLSTM(\n",
       "  (lstm): LSTM(93, 100, batch_first=True)\n",
       "  (h2o): Linear(in_features=100, out_features=93, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computing_device = get_computing_device()\n",
    "all_loaders, infos = build_all_loaders('../pa4Data/')\n",
    "\n",
    "char2ind = infos['char_2_index']\n",
    "ind2char = infos['index_2_char']\n",
    "\n",
    "model = BasicLSTM(len(char2ind), 100, len(char2ind))\n",
    "# model = MultiLayerLSTM(len(char2ind), 120, num_layers=7, num_output=len(char2ind))\n",
    "model.load_state_dict(torch.load('./lstm100_300epochs/model_state.pt', map_location='cpu'))\n",
    "model.eval()\n",
    "model.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_str = \"<start>\"\n",
    "prime_tensor = torch.zeros(len(prime_str), len(char2ind)).to(computing_device)\n",
    "        \n",
    "for i in range(len(prime_str)):\n",
    "    char = prime_str[i]\n",
    "    prime_tensor[i, char2ind[char]] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from a category and starting letter\n",
    "def sample(model, T=None, max_length = 2000, stop_on_end_tag=False):\n",
    "    \n",
    "    sample_music = \"\"\n",
    "    \n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        model.reset_hidden(computing_device)\n",
    "        \n",
    "        # Prime with <start>, hidden state is now ready\n",
    "        logits = model(prime_tensor.unsqueeze(dim=0))[-1]\n",
    "        \n",
    "        i = 0\n",
    "        while i < max_length:\n",
    "            res_ind = None\n",
    "            if T is None:\n",
    "                res_ind = np.argmax(logits).item()\n",
    "            else:\n",
    "                prob = np.array(F.softmax(logits/T, dim=0))\n",
    "                res_ind = np.random.choice(len(char2ind), 1, p=prob)[0]\n",
    "            final_char = ind2char[res_ind]            \n",
    "            sample_music += final_char\n",
    "            i+=1\n",
    "            if i % 50 == 0:\n",
    "                print(i)\n",
    "                \n",
    "            if stop_on_end_tag and (sample_music[-5:] == \"<end>\" or sample_music[-7:] == \"<start>\"):\n",
    "                print(\"Found <end>, stop making music at i = {0}.\".format(i))\n",
    "                break\n",
    "                \n",
    "            next_char_tensor = torch.zeros(len(char2ind)).to(computing_device)\n",
    "            next_char_tensor[res_ind] = 1\n",
    "            next_char_tensor = next_char_tensor.view(1,1,-1)\n",
    "            logits = model(next_char_tensor)[-1]\n",
    "        return sample_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = sample(model, T=1, max_length=2_000, stop_on_end_tag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X: 1\n",
      "T:Brellead Ornoin\n",
      "O:Frandensonde\n",
      "I:ab crads\n",
      "Z:id:hn-harn-25\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:A\n",
      "~E3 ADF|AF EF|B/B/A FDE||\n",
      "FA FE|FED F2|BF GB/A/|GF GF|ED DB/e/|dc d2:|\n",
      "\n",
      "\n",
      "X:56\n",
      "T:Seat W\\'acharkigh, The\n",
      "T:Flarx ul Flond\n",
      "Z:id:hn-polka-811\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:Am\n",
      "DE/D/ DF|BA F2|BB BG/A/|dB/A/ dB/2F/|1 G2G D:|2 AG A||\n",
      "B2 d>e d|g/f/d/c/ dd|ed d>d|eB GE|D>D GA|\n",
      "AG GG/E/|AB/c/ BG|FE E>A|Bd e>d|A^G A2:|2\n",
      "\n",
      "\n",
      "X:70\n",
      "T:Mattic Fart\\'O Tril's Mairolin\n",
      "R:jig\n",
      "D:Prady: Mdylye an Cuofly Cop\\'i Bran: The M Sores'e Real,\n",
      "O:Cranhe\n",
      "T:Noll ath Elvig \"Un Glurk or the Moog Spony-#21\n",
      "D:Deve to Alac.\n",
      "Z:id:hn-polka-33\n",
      "M:C|\n",
      "L:1/8\n",
      "K:Em\n",
      "|: B2d GFE | E3 EED | GED A2D | B2c Bc | d2c A2B | AGF G3A ||\n",
      "V: \n",
      "|: Ad | DFD DED | DED D2F | D2D F2 :|\n",
      "|: cFA efg | fdA BAG | ABc dcB | cA A2F | \n",
      "G3 | ~B3 A2d | g2f gfe | dcd FAF |1 A3- AA/ :|2 FED DEA | DFG A2 :|\n",
      "|: G3 | ~g3 ggd | b2g f2c | gfd cAF | fdc dcd |\n",
      "~c3 BAF |1 BAA BAG | d2c d2d | \n",
      "eAA BAF | def gfe | fef gfe | fAG A2f :|2 GAF G2e ||\n",
      "|: faf ac | deB dcB | ded cAF |1 B2c d2 :|2 cAG A2 || \n",
      "Peve fa |  a a aa a ~a2 af fA | A2c AGF | ~E3 G2A :|\n",
      "\n",
      "\n",
      "X:28\n",
      "T:Elsh Wallaige, The\n",
      "R:polka\n",
      "Z:id:hn-polka-59\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:G\n",
      "D2 e>g|ef g2|ef ed|cB AD|FE A2|B2 d2|GB BA|ED DE|1 c2::2 g2 g| f/a/ ge|f/a/g/e/ dd |\n",
      "g2 dB/A/|G/A/G/F/ ED|Bc AB|ce F/^A/|EB AG|G2 B/B/G/F/|1 AG GA:|2 d3:|\n",
      "|:Bc|BA | c/d/c/d/ BA|ED D>A|BG G>A|Bd G2|G>B de:|\n",
      "\n",
      "\n",
      "X:58\n",
      "T:McHopTeer m'ur Sthe\n",
      "T:Moree's Joonpipe\n",
      "R:march\n",
      "Z:id:hn-polka-23\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:D\n",
      "dB/A/ BB|BA FG|CE FE|FA A>c|dB c/B/AB|cA E2:|\n",
      "|:BA GB|ef e2|ea b/a/a|ge f/g/d/e/|fc de|f/e/d/e/ dB|1 A2 BG:|2 e2 BG||\n",
      "|:cB AG/G/|FA B>B|AF A/B/c|de fd|e>f gf|ec B/A/G/A/|1 GE GF:|2 a2 bg/g/||\n",
      "|:ef/f/ ga|gb ag|ga gf|ga ge|B/A/B/A/ B/A/g|1 E2 D2:|2 GA G3||\n",
      "|:gg g>g|f/g/f/d/ e/d/B/c/|dB G//F/|DF AB/c/|dB G//G/G/G/|\n",
      "D/C/C/D/ D>F|DA GE/F/E/F/|EG FG|A>B d2:|\n",
      "|:A/A/B/A/ e/d/d/c/|dB AB/c/|dB G2|a/f/d/B/ A/G/A|1 G2 G2:|2 dG G>A||\n",
      "\n",
      "\n",
      "X:22\n",
      "T:Wande treey Grey Boles: The Thee Harduner's\n",
      "R:polka\n",
      "Z:id:hn-polka-71\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:D\n",
      "GE/A/ BB|AG G\n"
     ]
    }
   ],
   "source": [
    "m1 = m1.replace(\"<end>\", \"\").replace(\"<start>\", \"\")\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = \"\"\"<start>\n",
    "X:67\n",
    "T:Ewwly PrevEundie, F Lage (1990-#17, #79, Thin's Foglen Spaistom\n",
    "Z:id:hn-polka-77\n",
    "M:2/4\n",
    "L:1/8\n",
    "K:G\n",
    "DB/G/ D2|GE EF|GE DG/E/|B,D/D/B, |\n",
    "G,E,3|G,2D ||\n",
    "|:dF d>B|A2 A2:|\n",
    "|:f2 d2|ef/g/ fe|a2 af|gf/e/ dB|AG F2|EF GA/B/|AB AB|GB F2:|\n",
    "|:df e/d/d|fB e/f/g|ag g>g|af ba|ba ga|ba g>f|ed ed|dg ag|ae ed|cd e/f/a|gb ag/e/|fA dc|e2 eA/A/|1 de d2:|2 B2 dd||\n",
    "<end>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = \"\"\"<start>\n",
    "X:58\n",
    "T:McHopTeer m'ur Sthe\n",
    "T:Moree's Joonpipe\n",
    "R:march\n",
    "Z:id:hn-polka-23\n",
    "M:2/4\n",
    "L:1/8\n",
    "K:D\n",
    "dB/A/ BB|BA FG|CE FE|FA A>c|dB c/B/AB|cA E2:|\n",
    "|:BA GB|ef e2|ea b/a/a|ge f/g/d/e/|fc de|f/e/d/e/ dB|1 A2 BG:|2 e2 BG||\n",
    "|:cB AG/G/|FA B>B|AF A/B/c|de fd|e>f gf|ec B/A/G/A/|1 GE GF:|2 a2 bg/g/||\n",
    "|:ef/f/ ga|gb ag|ga gf|ga ge|B/A/B/A/ B/A/g|1 E2 D2:|2 GA G3||\n",
    "|:gg g>g|f/g/f/d/ e/d/B/c/|dB G//F/|DF AB/c/|dB G//G/G/G/|\n",
    "D/C/C/D/ D>F|DA GE/F/E/F/|EG FG|A>B d2:|\n",
    "|:A/A/B/A/ e/d/d/c/|dB AB/c/|dB G2|a/f/d/B/ A/G/A|1 G2 G2:|2 dG G>A||\n",
    "<end>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_at_k(cur_str, lstm_out, k, save_fig=False, path_to_save=None, N_PER_ROW = 20):\n",
    "\n",
    "    def scale(x):\n",
    "        return ((x - x.min())/(x.max() - x.min())) * 2 - 1\n",
    "    lstm_out_k = lstm_out[:,k]\n",
    "\n",
    "    n_activations, n_hiddens = lstm_out.shape\n",
    "    n_activations = n_activations\n",
    "    target_length = int(np.ceil(n_activations / N_PER_ROW) * N_PER_ROW)\n",
    "    n_padding = target_length - n_activations\n",
    "\n",
    "    num_rows = target_length//N_PER_ROW\n",
    "    final_frame = np.array(list(scale(lstm_out_k.flatten())) + list(np.zeros(n_padding))).reshape((target_length//N_PER_ROW, N_PER_ROW))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    im = ax.imshow(final_frame, cmap='RdBu_r')\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(N_PER_ROW))\n",
    "    ax.set_yticks(np.arange(num_rows))\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(num_rows):\n",
    "        for j in range(N_PER_ROW):\n",
    "            true_pos = i * N_PER_ROW + j\n",
    "            if true_pos < len(cur_str):\n",
    "                char = cur_str[true_pos]\n",
    "                if char == \"\\n\":\n",
    "                    char = \"\\\\n\"\n",
    "                text = ax.text(j, i, char,\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontsize=15)\n",
    "\n",
    "    ax.set_title(\"Activation map\")\n",
    "\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig(os.path.join(path_to_save, 'heatmap-{0}.png'.format(k)), bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_all_heatmaps(sample, PATH_TO_SAVE = './figs', test_run=False, inds=[]):    \n",
    "    # Get Outputs from LSTM\n",
    "    cur_str = sample\n",
    "    input = torch.zeros(len(cur_str), len(char2ind)).to(computing_device)\n",
    "\n",
    "    for i in range(len(cur_str)-1):\n",
    "        char = cur_str[i]\n",
    "        input[i, char2ind[char]] = 1\n",
    "    input.unsqueeze_(0)\n",
    "    model.reset_hidden(computing_device)\n",
    "    lstm_out = model.lstm(input, model.hidden)[0][0].detach().numpy()\n",
    "\n",
    "    os.makedirs(PATH_TO_SAVE, exist_ok=True)\n",
    "    N_K = lstm_out.shape[1]\n",
    "    if test_run:\n",
    "        for k in inds:\n",
    "            heatmap_at_k(cur_str, lstm_out, k, save_fig=False, path_to_save=PATH_TO_SAVE)\n",
    "    else:\n",
    "        for k in tqdm(range(N_K)):\n",
    "            heatmap_at_k(cur_str, lstm_out, k, save_fig=True, path_to_save=PATH_TO_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = \"\"\"<start>\n",
    "X:71\n",
    "T:Urnd (er mant Macqunett an !\n",
    "R:polka\n",
    "Z:id:hn-polha-p217\n",
    "M:6/8\n",
    "L:1/8\n",
    "K:A\n",
    "AG D4|Bc A>B|cd FA|BA AB|AG FD|Bd f/e/|ce cG|FE FE:|\n",
    "D:|\n",
    "DF DF | FE FD FE | DD F,DA, |\n",
    "A2A c2A :|d f2g ffd | A2E D3B | BAF GAB |1 c2G F2D :|2 ED FG ||\n",
    "<end>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c39570ff534037883181946074e3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_and_save_all_heatmaps(m2, './figs_300', test_run=True, inds=[0, 1, 2, 44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_s = \"\"\n",
    "for c in all_loaders['train'].dataset.text_chunks:\n",
    "    all_s += c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
