{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40d717cfc2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../pa4Data/train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../pa4Data/val.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masters/cse253_pa4/renu/Dataloader.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, filepath, chunk_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0meach\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mhot\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         '''\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mhotencoded_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerized_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhotencoded_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumerized_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masters/cse253_pa4/renu/Dataloader.py\u001b[0m in \u001b[0;36m__encode_file\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mhotencoded_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hotencode_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mnumerized_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/masters/cse253_pa4/renu/Dataloader.py\u001b[0m in \u001b[0;36m__hotencode_char\u001b[0;34m(self, char)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from LSTM import LSTM\n",
    "import Dataloader\n",
    "\n",
    "model = LSTM(hidden_dim=100)\n",
    "\n",
    "#loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#, lr=learning_rate)\n",
    "\n",
    "num_epochs = 5  \n",
    "\n",
    "\n",
    "N = 50\n",
    "N_minibatch_loss = 0.0\n",
    "\n",
    "dataloader = Dataloader.Dataloader()\n",
    "\n",
    "inputs,targets = dataloader.load_data('../pa4Data/train.txt')\n",
    "\n",
    "val_inputs,val_targets = dataloader.load_data('../pa4Data/val.txt')\n",
    "\n",
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "val_losses = []\n",
    "epochs = []\n",
    "minibatches = []\n",
    "val_count=0\n",
    "save_state_dict=None\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    model.clear_hidden() # zero out hidden/memory state\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        model.train()\n",
    "        \n",
    "        # Zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs[i])\n",
    "        \n",
    "        loss = criterion(outputs, targets[i].long())\n",
    "          \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss.item()\n",
    "        \n",
    "        if i % N == 0: \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, i, N_minibatch_loss))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "            \n",
    "            # validation\n",
    "            print('...validating...')\n",
    "            val_loss=0.0\n",
    "            with torch.no_grad():\n",
    "                for v in range(len(val_inputs)):\n",
    "                    val_outputs = model(val_inputs[v])\n",
    "                    val_loss += criterion(val_outputs, val_targets[v]).item()\n",
    "            val_loss/=len(val_inputs)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            epochs.append(epoch+1)\n",
    "            minibatches.append(i)\n",
    "            \n",
    "            print('Epoch %d, average validation loss: %.3f' %\n",
    "                (epoch + 1, val_loss))\n",
    "            \n",
    "            # early stopping\n",
    "            if len(val_losses)>=2 and val_loss>val_losses[-2]:\n",
    "                if val_count==0:\n",
    "                    save_state_dict = model.state_dict()\n",
    "                val_count+=1\n",
    "                if val_count==5:\n",
    "                    break  \n",
    "            \n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "\n",
    "if save_state_dict:\n",
    "    PATH = \"./output/{}/best.pt\".format(model_name)\n",
    "    torch.save(save_state_dict, PATH)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#train_loss_df = {'epoch':epochs,'minibatch':minibatches,'loss':avg_minibatch_loss}\n",
    "#val_loss_df = {'epoch':epochs,'minibatch':minibatches,'loss':val_losses}\n",
    "\n",
    "train_loss_df = pd.DataFrame({'loss':avg_minibatch_loss})\n",
    "val_loss_df = pd.DataFrame({'loss':val_losses})\n",
    "\n",
    "model_name=\"lstm\" + str(model.hidden_dim)\n",
    "val_loss_file = \"./output/{}/validation_loss.csv\".format(model_name)\n",
    "train_loss_file = \"./output/{}/train_loss.csv\".format(model_name)\n",
    "scores_file = \"./output/{}/scores.csv\".format(model_name)\n",
    "\n",
    "train_loss_df.to_csv(train_loss_file)\n",
    "val_loss_df.to_csv(val_loss_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./output/{}/lstm.pt\".format(model_name)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average minibatch 0 loss: 0.090\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 4.530\n",
      "Epoch 1, average minibatch 50 loss: 3.955\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.622\n",
      "Epoch 1, average minibatch 100 loss: 3.538\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.628\n",
      "Epoch 1, average minibatch 150 loss: 3.553\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.731\n",
      "Epoch 1, average minibatch 200 loss: 3.453\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.555\n",
      "Epoch 1, average minibatch 250 loss: 3.384\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.438\n",
      "Epoch 1, average minibatch 300 loss: 3.199\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.394\n",
      "Epoch 1, average minibatch 350 loss: 3.106\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.266\n",
      "Epoch 1, average minibatch 400 loss: 2.862\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.192\n",
      "Epoch 1, average minibatch 450 loss: 2.775\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.189\n",
      "Epoch 1, average minibatch 500 loss: 2.735\n",
      "...validating...\n",
      "Epoch 1, average validation loss: 3.142\n",
      "Finished 1 epochs of training\n",
      "Epoch 2, average minibatch 0 loss: 1.863\n",
      "...validating...\n",
      "Epoch 2, average validation loss: 3.113\n",
      "Epoch 2, average minibatch 50 loss: 2.636\n",
      "...validating...\n",
      "Epoch 2, average validation loss: 2.909\n",
      "Epoch 2, average minibatch 100 loss: 2.437\n",
      "...validating...\n",
      "Epoch 2, average validation loss: 2.945\n",
      "Epoch 2, average minibatch 150 loss: 2.579\n",
      "...validating...\n",
      "Epoch 2, average validation loss: 2.951\n",
      "Epoch 2, average minibatch 200 loss: 2.225\n",
      "...validating...\n",
      "Epoch 2, average validation loss: 2.989\n",
      "Finished 2 epochs of training\n",
      "Epoch 3, average minibatch 0 loss: 0.048\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.954\n",
      "Epoch 3, average minibatch 50 loss: 2.255\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.802\n",
      "Epoch 3, average minibatch 100 loss: 2.107\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.850\n",
      "Epoch 3, average minibatch 150 loss: 2.359\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.876\n",
      "Epoch 3, average minibatch 200 loss: 1.984\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.862\n",
      "Epoch 3, average minibatch 250 loss: 2.075\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.735\n",
      "Epoch 3, average minibatch 300 loss: 1.918\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.797\n",
      "Epoch 3, average minibatch 350 loss: 1.828\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.803\n",
      "Epoch 3, average minibatch 400 loss: 1.683\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.824\n",
      "Epoch 3, average minibatch 450 loss: 1.807\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.857\n",
      "Epoch 3, average minibatch 500 loss: 1.959\n",
      "...validating...\n",
      "Epoch 3, average validation loss: 2.888\n",
      "Finished 3 epochs of training\n",
      "Epoch 4, average minibatch 0 loss: 1.198\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.903\n",
      "Epoch 4, average minibatch 50 loss: 1.902\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.711\n",
      "Epoch 4, average minibatch 100 loss: 1.788\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.753\n",
      "Epoch 4, average minibatch 150 loss: 2.084\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.734\n",
      "Epoch 4, average minibatch 200 loss: 1.713\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.841\n",
      "Epoch 4, average minibatch 250 loss: 1.820\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.662\n",
      "Epoch 4, average minibatch 300 loss: 1.641\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.764\n",
      "Epoch 4, average minibatch 350 loss: 1.588\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.772\n",
      "Epoch 4, average minibatch 400 loss: 1.461\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.788\n",
      "Epoch 4, average minibatch 450 loss: 1.604\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.912\n",
      "Epoch 4, average minibatch 500 loss: 1.776\n",
      "...validating...\n",
      "Epoch 4, average validation loss: 2.803\n",
      "Finished 4 epochs of training\n",
      "Epoch 5, average minibatch 0 loss: 1.057\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.894\n",
      "Epoch 5, average minibatch 50 loss: 1.738\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.685\n",
      "Epoch 5, average minibatch 100 loss: 1.638\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.724\n",
      "Epoch 5, average minibatch 150 loss: 1.920\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.774\n",
      "Epoch 5, average minibatch 200 loss: 1.590\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.829\n",
      "Epoch 5, average minibatch 250 loss: 1.729\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.697\n",
      "Epoch 5, average minibatch 300 loss: 1.593\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.766\n",
      "Epoch 5, average minibatch 350 loss: 1.477\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.754\n",
      "Epoch 5, average minibatch 400 loss: 1.353\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.781\n",
      "Epoch 5, average minibatch 450 loss: 1.501\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.946\n",
      "Epoch 5, average minibatch 500 loss: 1.667\n",
      "...validating...\n",
      "Epoch 5, average validation loss: 2.749\n",
      "Finished 5 epochs of training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        loss\n",
       " 0   0.090478\n",
       " 1   3.955264\n",
       " 2   3.537617\n",
       " 3   3.553275\n",
       " 4   3.453483\n",
       " 5   3.383699\n",
       " 6   3.199141\n",
       " 7   3.106257\n",
       " 8   2.862025\n",
       " 9   2.775416\n",
       " 10  2.735421\n",
       " 11  1.863143\n",
       " 12  2.636249\n",
       " 13  2.436724\n",
       " 14  2.578782\n",
       " 15  2.224940\n",
       " 16  0.048423\n",
       " 17  2.254766\n",
       " 18  2.106628\n",
       " 19  2.358689\n",
       " 20  1.983901\n",
       " 21  2.075039\n",
       " 22  1.917606\n",
       " 23  1.827750\n",
       " 24  1.682531\n",
       " 25  1.806865\n",
       " 26  1.959133\n",
       " 27  1.198345\n",
       " 28  1.902451\n",
       " 29  1.787740\n",
       " 30  2.083501\n",
       " 31  1.713251\n",
       " 32  1.819865\n",
       " 33  1.641254\n",
       " 34  1.587984\n",
       " 35  1.461400\n",
       " 36  1.604349\n",
       " 37  1.775952\n",
       " 38  1.057153\n",
       " 39  1.738091\n",
       " 40  1.638457\n",
       " 41  1.920360\n",
       " 42  1.590486\n",
       " 43  1.729009\n",
       " 44  1.593388\n",
       " 45  1.476837\n",
       " 46  1.353311\n",
       " 47  1.500835\n",
       " 48  1.667480,         loss\n",
       " 0   4.530399\n",
       " 1   3.622205\n",
       " 2   3.628471\n",
       " 3   3.730939\n",
       " 4   3.554993\n",
       " 5   3.437960\n",
       " 6   3.394003\n",
       " 7   3.265923\n",
       " 8   3.191975\n",
       " 9   3.189005\n",
       " 10  3.142303\n",
       " 11  3.112886\n",
       " 12  2.908919\n",
       " 13  2.945286\n",
       " 14  2.950505\n",
       " 15  2.988925\n",
       " 16  2.954124\n",
       " 17  2.801696\n",
       " 18  2.850075\n",
       " 19  2.876199\n",
       " 20  2.861722\n",
       " 21  2.734794\n",
       " 22  2.797392\n",
       " 23  2.802795\n",
       " 24  2.823858\n",
       " 25  2.856652\n",
       " 26  2.887527\n",
       " 27  2.902604\n",
       " 28  2.711208\n",
       " 29  2.752653\n",
       " 30  2.733706\n",
       " 31  2.840607\n",
       " 32  2.662028\n",
       " 33  2.764433\n",
       " 34  2.772412\n",
       " 35  2.788055\n",
       " 36  2.912419\n",
       " 37  2.802652\n",
       " 38  2.894471\n",
       " 39  2.685358\n",
       " 40  2.724134\n",
       " 41  2.773578\n",
       " 42  2.829259\n",
       " 43  2.697100\n",
       " 44  2.765714\n",
       " 45  2.754473\n",
       " 46  2.780978\n",
       " 47  2.946126\n",
       " 48  2.749294)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import train \n",
    "from LSTM import LSTM\n",
    "import Dataloader\n",
    "import torch\n",
    "\n",
    "\n",
    "model = LSTM(hidden_dim=100)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#, lr=learning_rate)\n",
    "EPOCHS = 5\n",
    "output_dir='lstm100_5epoch'\n",
    "\n",
    "\n",
    "dataloader = Dataloader.Dataloader()\n",
    "train_inputs,train_targets = dataloader.load_data('../pa4Data/test.txt')\n",
    "val_inputs,val_targets = dataloader.load_data('../pa4Data/val.txt')\n",
    "\n",
    "\n",
    "train(model, criterion, optimizer, train_inputs, train_targets, val_inputs, val_targets, output_dir, num_epochs=EPOCHS)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5991346127108526"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
